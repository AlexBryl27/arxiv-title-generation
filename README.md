# arxiv-title-generation

Последнее задание курса "Нейронные сеит и обработка текста" на степике https://stepik.org/lesson/261085/step/1

В задаче предлагается по предложенному фрагменту статьи сгенерировать заголовок. В качестве решения я использую предобученную модель BERT в качестве автокодировщика (одна и та же модель для энкодера и декодера). Эту модель я дообучаю на имеющихся данных.

Пара замечаний:

1. Из России оказалось проблемным купить себе подписку на colab pro, либо арендовать сервер с GPU на google cloud platform (другие решения я пока не рассматривал). Поэтому некоторые параметры обучения (максимальная длина последовательности, размер батча, количество эпох) было обусловлено не оптимальным выбором, а ограничениями пратформы colab.

2. Оказалось, что часть статей из тестовых данных встречается в тренировочных. Чтобы не делать лишних предсказаний, в качестве тестового файла используется файл sample.csv, с уже проставленными некоторыми заголовками. В этом ноутбуке работа с дубликатами не показана.

Результат: обучив всего одну эпоху модели (больше не позволяла платформа по времени сессии) я получил f1-score около 52 % в соответсвующем соревновании на kaggle https://www.kaggle.com/c/title-generation/leaderboard
